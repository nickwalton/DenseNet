# -*- coding: utf-8 -*-
"""DenseNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tv3dxRris3VfwZPwQtqKM9g8bZyVaj6h
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, utils, datasets


# Implemented DenseNet from https://arxiv.org/pdf/1608.06993.pdf
class DenseBlock(nn.Module):
    def __init__(self, size, n_layers, k0, k):
        super(DenseBlock, self).__init__()
        self.activation = nn.ReLU()
        self.n_layers = n_layers
        self.layers = nn.ModuleList()

        for i in range(n_layers):
            batch_norm = nn.BatchNorm2d(num_features=k0+k*i).cuda()
            bottleneck = nn.Conv2d(k0+k*i, 4*k, kernel_size=1, stride=1, padding=0).cuda()
            conv = nn.Conv2d(4*k, k, kernel_size=3, stride=1, padding=1).cuda()
            self.layers.extend([batch_norm, self.activation, bottleneck, conv])

    def forward(self, x):
        state = x
        i = 0
        while i < self.n_layers:
            l1 = self.layers[i](state)
            l2 = self.layers[i+1](l1)
            l3 = self.layers[i+2](l2)
            l4 = self.layers[i+3](l3)
            state = torch.cat([state, l4], dim=1)
            i += 4

        return state


class DenseNet(nn.Module):
    def __init__(self, input_size, output_size, n_layers):
        super(DenseNet, self).__init__()
        k0 = 1
        k = 4
        self.fc_size = 14*14*4

        self.dense_block = DenseBlock(input_size, n_layers=n_layers, k0=k0, k=k)
        self.transition_conv = nn.Conv2d(k+k0, 4, kernel_size=1, stride=1, padding=0).cuda()
        self.transition_pool = nn.AvgPool2d(kernel_size=2, stride=2).cuda()
        self.final_fc = nn.Linear(self.fc_size, output_size).cuda()
        self.softmax = nn.Softmax()

        self.net = nn.Sequential(self.dense_block, self.transition_conv,
                                 self.transition_pool, self.final_fc, self.softmax)

    def forward(self, x):
        l1 = self.dense_block(x)
        l2 = self.transition_conv(l1)
        l3 = self.transition_pool(l2)

        l4 = self.final_fc(l3.view(-1, self.fc_size))
        l5 = self.softmax(l4)
        return l5


def train(args, model, train_loader, optimizer, epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to("cuda"), target.to("cuda")
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()


def test(args, model, test_loader):
    model.eval()
    test_loss = 0
    correct = 0
    num = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to("cuda"), target.to("cuda")
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()
            num += pred.shape[0]
        print("Correct: ", str(correct/num*100)+"%")


if __name__ == '__main__':
    batch_size = 128
    args = dict()
    args["epochs"] = 10
    args["log_interval"] = 50
    args["lr"] = 1e-3

    train_loader = torch.utils.data.DataLoader(
        datasets.MNIST('./data', train=True, download=True,
        transform=transforms.Compose([transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))])),
        batch_size=batch_size, shuffle=True)

    test_loader = torch.utils.data.DataLoader(
        datasets.MNIST('./data', train=False, transform=transforms.Compose([
        transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])),
        batch_size=batch_size, shuffle=True)

    model = DenseNet(28, 10, 4)
    optimizer = optim.SGD(model.parameters(), lr=args["lr"])

    for epoch in range(1, args["epochs"] + 1):
        train(args, model, train_loader, optimizer, epoch)
        test(args, model, test_loader)

